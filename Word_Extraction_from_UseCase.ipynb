{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9INKmOv48WpZ",
        "outputId": "91cecf7b-e985-4a83-c18c-ebc2e5566403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fuzzywuzzy\n"
      ],
      "metadata": {
        "id": "aC1bgtjE9Hit"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "V3nRoZ7C8Nee",
        "outputId": "925f22a5-c511-43ed-a15c-f99dfb69f960"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fuzzywuzzy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-891a90729d1c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfuzzywuzzy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Заданный список слов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m words = [\"EMC\", \"Doors\", \"Steering mechanism\", \"Braking\", \"Safety belt\",\n\u001b[1;32m      5\u001b[0m          \u001b[0;34m\"Seats\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Audible warning devices\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Speedometer and odometer\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from fuzzywuzzy import process\n",
        "\n",
        "# Заданный список слов\n",
        "words = [\"EMC\", \"Doors\", \"Steering mechanism\", \"Braking\", \"Safety belt\",\n",
        "         \"Seats\", \"Audible warning devices\", \"Speedometer and odometer\",\n",
        "         \"Steering equipment\", \"Heating system\", \"AVAS\", \"wipe and wash\"]\n",
        "\n",
        "# Текст, в котором нужно искать слова\n",
        "text = \"\"\"\n",
        "I-24205\n",
        "OC Section: [F-7600] New OC Section: 09 Navigation and Entertainment / Acoustic Vehicle Alerting System (AVAS)\n",
        "Description:\n",
        "Title: Vehicle Alerting System fault\n",
        "Goal: To provide granted indications that AVAS function has problems\n",
        "Scope: AVAS and other car systems\n",
        "Actors: N/A\n",
        "Preconditions: AVAS operating normally\n",
        "Trigger: AVAS has malfunction\n",
        "Main scenario: N/A\n",
        "Postconditions:\n",
        "- SWP out_2 displays msg\n",
        "- Push notification inside Atom mobile app out_5 and out_44 ATOM HUB\n",
        "Priority: Normal\n",
        "Type: Use Case CF\n",
        "\"\"\"\n",
        "\n",
        "# Получаем список слов из текста\n",
        "found_words = set()\n",
        "for word in words:\n",
        "    result = process.extractOne(word, text.split(), score_cutoff=90)\n",
        "    if result:\n",
        "        found_words.add(word)\n",
        "\n",
        "print(\"Найденные слова:\", found_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentence Transformer**"
      ],
      "metadata": {
        "id": "vMWRVagk9Lst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Модель для преобразования предложений в эмбеддинги\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Заданный список слов/фраз\n",
        "reglaments_names = ['EMC', 'AVAS', 'Braking system']\n",
        "\n",
        "words = [\"Doors\", \"Steering mechanism\", \"Safety belt\",\n",
        "         \"Seats\", \"Audible warning devices\", \"Speedometer and odometer\",\n",
        "         \"Steering equipment\", \"Heating system\", \"wipe and wash\"]\n",
        "\n",
        "# Текст, в котором нужно искать слова\n",
        "text = \"\"\"\n",
        "Steering mechanism\n",
        "Safety belt\n",
        "\"\"\"\n",
        "\n",
        "# Преобразуем текст и слова в векторы\n",
        "text_embedding = model.encode(text)\n",
        "words_embeddings = model.encode(words)\n",
        "\n",
        "# Порог схожести\n",
        "threshold = 0.25\n",
        "\n",
        "# Находим слова, схожие с текстом\n",
        "reglament_found_names = set()\n",
        "for word, word_embedding in zip(words, words_embeddings):\n",
        "    similarity = util.cos_sim(text_embedding, word_embedding)\n",
        "    if similarity > threshold:\n",
        "        found_words.add(word)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LERNriGE8Xwd",
        "outputId": "cf8ad3fe-384d-4ee3-daed-c403b48939df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найденные слова: {'Steering mechanism', 'Steering equipment', 'Safety belt'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Spacy**"
      ],
      "metadata": {
        "id": "tnTgf1uz99W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Загружаем модель spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Заданный список слов\n",
        "words = [\"EMC\", \"Doors\", \"Steering mechanism\", \"Braking\", \"Safety belt\",\n",
        "         \"Seats\", \"Audible warning devices\", \"Speedometer and odometer\",\n",
        "         \"Steering equipment\", \"Heating system\", \"AVAS\", \"wipe and wash\"]\n",
        "\n",
        "# Приводим список слов к нижнему регистру для сопоставления\n",
        "words_set = set(word.lower() for word in words)\n",
        "\n",
        "# Текст, в котором нужно искать слова\n",
        "text = \"\"\"\n",
        "Steering mechanism\n",
        "Safety belt\n",
        "\"\"\"\n",
        "\n",
        "# Обработка текста с помощью spaCy\n",
        "doc = nlp(text.lower())\n",
        "\n",
        "# Извлечение лемм (основных форм слов) и текстов из токенов\n",
        "found_words = set()\n",
        "for word in words_set:\n",
        "    if chunk.text in words_set:\n",
        "        found_words.add(chunk.text)\n",
        "\n",
        "print(\"Найденные слова:\", found_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTNo1e449Sfd",
        "outputId": "ccacfba6-2788-4cb6-fbd5-fa282e8ca296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найденные слова: set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UysSTG-g-D4c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}